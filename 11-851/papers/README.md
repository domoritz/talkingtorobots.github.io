By popular request, I've decided to occasionally dump some links here.

Planning
----
- [LLM Powered Autonomous Agents (blog)](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [LaMPP: Language Models as Probabilistic Priors for Perception and Action](https://arxiv.org/abs/2302.02801)
- [LLM+P: Empowering Large Language Models with Optimal Planning Proficiency](https://arxiv.org/abs/2304.11477)
- [Do Embodied Agents Dream of Pixelated Sheep?: Embodied Decision Making using Language Guided World Modelling](https://arxiv.org/abs/2301.12050)
- [Grounding Classical Task Planners via Vision-Language Models](https://arxiv.org/abs/2304.08587)

Navigation
----
- [ViNT: A Foundation Model for Visual Navigation](https://visualnav-transformer.github.io/)
- [SACSoN: Scalable Autonomous Data Collection for Social Navigation](https://sites.google.com/view/sacson-review/home)
- [A System for Generalized 3D Multi-Object Search](https://arxiv.org/abs/2303.03178)
- [CoWs on Pasture: Baselines and Benchmarks for Language-Driven Zero-Shot Object Navigation](https://arxiv.org/abs/2203.10421)
- [Principles and Guidelines for Evaluating Social Robot Navigation Algorithms](https://arxiv.org/abs/2306.16740)

Manipulation
----
- [RVT: Robotic View Transformer for 3D Object Manipulation](https://robotic-view-transformer.github.io/)
- [Physically Grounded Vision-Language Models for Robotic Manipulation](https://sites.google.com/view/physically-grounded-vlms)
- [LATTE: LAnguage Trajectory TransformEr](https://arxiv.org/abs/2208.02918)
- [LIV: Language-Image Representations and Rewards for Robotic Control](https://penn-pal-lab.github.io/LIV/)
- [Gesture-Informed Robot Assistance via Foundation Models](https://sites.google.com/view/giraf23/home)
- [SPRINT: Semantic Policy Pre-training via Language Instruction Relabeling](https://clvrai.github.io/sprint/)
- [Language-Driven Representation Learning for Robotics](https://arxiv.org/abs/2302.12766)
- [EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought](https://embodiedgpt.github.io/)
- [VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models](https://voxposer.github.io/)
- [VIMA: General Robot Manipulation with Multimodal Prompts](https://arxiv.org/abs/2210.03094)
- [Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks](https://deltaco-robot.github.io/)

Mobile Manipulation
----
- [Grounded Decoding: Guiding Text Generation with Grounded Models for Robot Control](https://grounded-decoding.github.io/)
- [Open-World Object Manipulation using Pre-Trained Vision-Language Models](https://robot-moo.github.io/)
- [Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models ](https://instructionaugmentation.github.io/)
- [HomeRobot: Open-Vocabulary Mobile Manipulation](https://arxiv.org/abs/2306.11565)
- [LSC: Language-guided Skill Coordination for Open-Vocabulary Mobile Pick-and-Place](https://languageguidedskillcoordination.github.io/)
- [Spatial-Language Attention Policies](https://robotslap.github.io/)
- [TidyBot: Personalized Robot Assistance with Large Language Models](https://tidybot.cs.princeton.edu/)
- [SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Task Planning](https://sayplan.github.io/)

Language to Motion
----
- [Language to Reward for Robot Skill Synthesis](https://language-to-reward.github.io/)
- [Text2Motion: From Natural Language Instructions to Feasible Plans](https://sites.google.com/stanford.edu/text2motion)
- [SayTap: Language to Quadrupedal Locomotion](https://saytap.github.io/)

Multi-Platform
----
- [ChatGPT for Robotics: Design Principles and Model Abilities](https://arxiv.org/abs/2306.17582)

Dialogue and QA
----
- [SEAGULL: An Embodied Agent for Instruction Following through Situated Dialog](https://sled.eecs.umich.edu/publication/zhang-2023-simbot/)
- [SQA3D: Situated Question Answering in 3D Scenes](https://sqa3d.github.io/)
- [Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners ](https://robot-help.github.io/)

Other
----
- [Large Language Models as General Pattern Machines](https://general-pattern-machines.github.io/)
- [Language to Rewards for Robotic Skill Synthesis](https://language-to-reward.github.io/)
- [Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?](https://eai-vc.github.io/)
- [MotionGPT](https://motion-gpt.github.io/)
- [Modeling Dynamic Environments with Scene Graph Memory](https://arxiv.org/abs/2305.17537)
- [Affordances from Human Videos as a Versatile Representation for Robotics](https://robo-affordances.github.io/)
- [RoboCat: A self-improving robotic agent](https://www.deepmind.com/blog/robocat-a-self-improving-robotic-agent)
- [ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification](https://zoom.taesiri.ai/)
- [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)
- [Kosmos-2: Grounding Multimodal Large Language Models to the World](https://arxiv.org/abs/2306.14824)
- [Behavior Transformers: Cloning k modes with one stone](https://mahis.life/bet/)
- [From Play to Policy: Conditional Behavior Generation from Uncurated Robot Data](https://play-to-policy.github.io/)
- [Affordance Diffusion: Synthesizing Hand-Object Interactions](https://judyye.github.io/affordiffusion-www/)
- [Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment](ut-austin-rpl.github.io/sirius)
- [https://sites.google.com/view/robot-r3m/](R3M: A Universal Visual Representation for Robot Manipulation)
